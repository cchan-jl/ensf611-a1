{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 2: Linear Models and Validation Metrics (30 marks total)\n",
    "### Due: October 10 at 11:59pm\n",
    "\n",
    "### Name: Carrie Chan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 1: Classification (14.5 marks total)\n",
    "\n",
    "You have been asked to develop code that can help the user determine if the email they have received is spam or not. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c6fc8",
   "metadata": {},
   "source": [
    "### Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33f86925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
    "\n",
    "Use the yellowbrick function `load_spam()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33583c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of X is:  (4600, 57) ,and the type is:  <class 'pandas.core.frame.DataFrame'>\n",
      "The size of y is:  (4600,) ,and the type is:  <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "# TO DO: Print size and type of X and y\n",
    "\n",
    "from yellowbrick.datasets import load_spam\n",
    "X, y = load_spam()\n",
    "\n",
    "print(\"The size of X is: \", X.shape, \",and the type is: \", type(X))\n",
    "print(\"The size of y is: \", y.shape, \",and the type is: \", type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e7204f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in X:  0\n",
      "Missing values in y:  0\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "print(\"Missing values in X: \", X.isnull().sum().sum())\n",
    "print(\"Missing values in y: \", y.isnull().sum())\n",
    "\n",
    "#There are no missing values in the dataset since both is.null.sum() resulted in zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489285a",
   "metadata": {},
   "source": [
    "For this task, we want to test if the linear model would still work if we used less data. Use the `train_test_split` function from sklearn to create a new feature matrix named `X_small` and a new target vector named `y_small` that contain **5%** of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9bc4a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Create X_small and y_small \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_big, X_small, y_big, y_small = train_test_split(X, y, random_state = 0, test_size = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `LogisticRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Implement the machine learning model with three different datasets: \n",
    "    - `X` and `y`\n",
    "    - Only first two columns of `X` and `y`\n",
    "    - `X_small` and `y_small`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f3d84",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the training and validation accuracy for the three different tests implemented in Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352106a3",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be4b5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Data Size  Training Accuracy  Validation Accuracy\n",
      "X and y              (4600, 57)           0.928116             0.938261\n",
      "2 col of X and y      (4600, 2)           0.608406             0.613043\n",
      "X_small and y_small   (230, 57)           0.965116             0.793103\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(max_iter=2000)\n",
    "\n",
    "X2 = X.iloc[:, :2]\n",
    "X_list = [X, X2, X_small]\n",
    "y_list = [y, y, y_small]\n",
    "\n",
    "results = pd.DataFrame(columns = ['Data Size', 'Training Accuracy', 'Validation Accuracy'])\n",
    "\n",
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_list[i], y_list[i], random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    val_accuracy = model.score(X_test, y_test)\n",
    "    train_accuracy = model.score(X_train, y_train)\n",
    "    results.loc[i] =  [X_list[i].shape, train_accuracy, val_accuracy]\n",
    "\n",
    "results.index = ['X and y', '2 col of X and y', 'X_small and y_small']\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4427d4f",
   "metadata": {},
   "source": [
    "### Questions (4 marks)\n",
    "1. How do the training and validation accuracy change depending on the amount of data used? Explain with values.\n",
    "2. In this case, what do a false positive and a false negative represent? Which one is worse?\n",
    "\n",
    "Answers:\n",
    "1. Both training accuracy and validation accuracy dropped significantly when the number of features used in the data was reduced to two. From the original dataset to using only two columns of X (reduced features), the training accuracy dropped from 0.928 to 0.608 and the validation accuracy dropped from 0.938 to 0.613. This may be due to the fact that the model was unable to make accurate predictions from important information being removed. The features removed from the dataset may have had information about patterns or relationships with the target variable.\n",
    "\n",
    "    When comparing the original dataset X with X_small (5% of the original dataset's rows), the training accuracy increased from 0.928 to 0.965, while the validation accuracy decreased from 0.938 to 0.793.The model may be overfitting with the small dataset as the training score is relatively high, while the validation score is relatively low (high variance).\n",
    "    \n",
    "2. In the case of this dataset, a false positive is when the model incorrectly marks a good email as spam, while a false negative is when the model incorrectly marks spam email as a good email. A false positive is worse in this scenario since losing an important email as a result of it being incorrectly marked and deleted is worse than spam being passed through the filter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7559517a",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe687f",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\n",
    "\n",
    "1. Code was sourced from lecture slides, lecture code examples, and labs.\n",
    "2. The steps were completed in chronological order.\n",
    "3. Generative AI was used to help search how to change the index of a dataframe. The prompt used was: \"How do I change the index of a pandas dataframe?\". I was able to change the code with the suggested methods and assign names to the index instead of having a numbered index for results.\n",
    "4. The lecure code examples and labs helped tremendously in completeing this part without any challenges. \n",
    "\n",
    "Sources:\n",
    "Generative AI to look up python pandas syntax as mentioned above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c78a8",
   "metadata": {},
   "source": [
    "## Part 2: Regression (10.5 marks total)\n",
    "\n",
    "For this section, we will be evaluating concrete compressive strength of different concrete samples, based on age and ingredients. You will need to repeat the steps 1-4 from Part 1 for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ba83c5",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ff2e34f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of X is:  (1030, 8) ,and the type is:  <class 'pandas.core.frame.DataFrame'>\n",
      "The size of y is:  (1030,) ,and the type is:  <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "# TO DO: Print size and type of X and y\n",
    "from yellowbrick.datasets import load_concrete\n",
    "X, y = load_concrete()\n",
    "\n",
    "print(\"The size of X is: \", X.shape, \",and the type is: \", type(X))\n",
    "print(\"The size of y is: \", y.shape, \",and the type is: \", type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5294cfa",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "693c5fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in X:  0\n",
      "Missing values in y:  0\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "print(\"Missing values in X: \", X.isnull().sum().sum())\n",
    "print(\"Missing values in y: \", y.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc60489",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model (1 mark)\n",
    "\n",
    "1. Import `LinearRegression` from sklearn\n",
    "2. Instantiate model `LinearRegression()`.\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5041945",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "lr = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de28482",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model (1 mark)\n",
    "\n",
    "Calculate the training and validation accuracy using mean squared error and R2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "970c038b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# MSE calculations\n",
    "linear_pred_val = lr.predict(X_test)\n",
    "linear_mse_val = mean_squared_error(y_test, linear_pred_val)\n",
    "linear_pred_train = lr.predict(X_train)\n",
    "linear_mse_train = mean_squared_error(y_train, linear_pred_train)\n",
    "\n",
    "# R2 calculations\n",
    "R2_train_score = lr.score(X_train, y_train)\n",
    "R2_val_score = lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa7795",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (1 mark)\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: MSE and R2 score\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88d223f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Training Accuracy Validation Accuracy\n",
      "MSE             111.358439           95.904136\n",
      "R2 Score          0.610823            0.623414\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "results = pd.DataFrame(index = ['MSE', 'R2 Score'], columns = ['Training Accuracy', 'Validation Accuracy'])\n",
    "results.loc['MSE'] = [linear_mse_train, linear_mse_val]\n",
    "results.loc['R2 Score'] =  [R2_train_score, R2_val_score]\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a42bda",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "1. Did using a linear model produce good results for this dataset? Why or why not?\n",
    "\n",
    "Answer:\n",
    "\n",
    "Using a linear model produced may not be a good fit for this dataset. The R2 scores for training and validation accuracy are both above 0.6 and may show a generally decent fit (source listed below) since the model accounted for a decent portion of the variance from the mean. However, the MSE numbers can be optimized to be lower as it may be considered high for this dataset's scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0ff2f",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb0880",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\n",
    "1. Code was sourced from lecture slides, lecture code examples, and labs.\n",
    "2. The steps were completed in chronological order.\n",
    "3. Generative AI was used to research about MSE and R2 score. The prompts used were: \"What is mean squared error for linear regression?\", \"What does R2 do for regression metrics?\". Code was not modified as generative AI was used to explain theory. \n",
    "4. I did not have any challenges in coding this portion as the lecture code examples provided a good guidline to this problem. There were some challenges in understanding R2 score and MSE, so generative AI and sources online were used to further help explain this topic.  \n",
    "\n",
    "Sources:\n",
    "https://towardsdatascience.com/an-ode-to-r-squared-804d8d0ed22c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ac3eb",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "*ADD YOUR FINDINGS HERE*\n",
    "\n",
    "An observed pattern in the results was that when the dataset had a relatively large sample size and a small number of attributes, the training accuracy and validation accuracy numbers were relatively low. This was indicated in Part 1 Questions and may be due to the fact that some of the useful features were removed that would have helped make predictions. As mentioned during lectures, linear models often perform well when the number of features is large compared to the number of samples. This is shown with the spam dataset which had a sample size of 4600 and 57 features, and had high training and validation accuracies (X and y dataset: Training accuracy = 0.928116, Validation accuracy = 0.938261). The analysis with the original dataset (X) and the dataset with only 2 columns (X2) had low variance between the training and validation accuracies as well (2 columns of X and y: Training accuracy = 0.608406, Validation accuracy = 0.613043). This would indicate that the model is not being overfitted. As indicated in the lecture slides, other models (instead of linear models) might yield better generalization performance in lower-dimensional spaces.\n",
    "\n",
    "It was also observed that the small dataset (5% of X and 5% y) had high variance between the training and validation accuracies, with a low validation accuracy score (X_small and y_small: Training accuracy = 0.965116, Validation accuracy = 0.793103). The high training accuracy indicates that the model effectively learned the patterns and relationships within the training data. However, the low validation accuracy indicates that the model cannot make accurate predictions with unseen data. \n",
    "\n",
    "In Part 2, the MSE numbers for training accuracy and validation accuracy may be relatively high for the concrete dataset since the target variable values are all below 100 (y.max() = 82.5992248). The validation score was also far from the maximum (R2 validation score = 0.623414, with maximum = 1.0), with high bias between training and validation accuracy (R2 training accuracy score = 0.610823, which is similar to the R2 validation score). Thus, other models may produce better results instead of a linear model as the R2 scores indicates underfitting of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b84eed",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*\n",
    "\n",
    "I liked that this assignment gave me more practice on the topics learned in class. This assignment helped me have a better understanding on how to implement linear models and use validation metrics. I found it confusing when interpreting the metrics for linear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db951b3a",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (4 marks)\n",
    "\n",
    "Repeat Part 2 with Ridge and Lasso regression to see if you can improve the accuracy results. Which method and what value of alpha gave you the best R^2 score? Is this score \"good enough\"? Explain why or why not.\n",
    "\n",
    "**Remember**: Only test values of alpha from 0.001 to 100 along the logorithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47623d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Ridge Regression: \n",
      "             Train_Acc_R2  Val_Acc_R2  Train_Acc_MSE  Val_Acc_MSE\n",
      "alpha = 1        0.610823    0.623415     111.358439    95.904035\n",
      "alpha = 100      0.610823    0.623453     111.358548    95.894268\n",
      "\n",
      "Results for Lasso Regression: \n",
      "            Train_Acc_R2  Val_Acc_R2  Train_Acc_MSE  Val_Acc_MSE\n",
      "alpha = 1       0.610609    0.624669     111.419648    95.584678\n",
      "alpha = 10      0.604314    0.626774     113.220780    95.048607\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Alpha values\n",
    "ridge_alpha_val = [1, 100]\n",
    "lasso_alpha_val = [1, 10]\n",
    "\n",
    "# Create dataframes\n",
    "results_ridge = pd.DataFrame(columns = ['Train_Acc_R2', 'Val_Acc_R2', 'Train_Acc_MSE', 'Val_Acc_MSE'])\n",
    "results_lasso = pd.DataFrame(columns = ['Train_Acc_R2', 'Val_Acc_R2', 'Train_Acc_MSE', 'Val_Acc_MSE'])\n",
    "\n",
    "# Ridge regression\n",
    "for i  in ridge_alpha_val:\n",
    "    ridge = Ridge(alpha = i).fit(X_train, y_train)\n",
    "    ridge_train_score = ridge.score(X_train, y_train)\n",
    "    ridge_val_score = ridge.score(X_test, y_test)\n",
    "    ridge_pred_train = ridge.predict(X_train)\n",
    "    ridge_train_mse = mean_squared_error(y_train, ridge_pred_train)\n",
    "    ridge_pred_val = ridge.predict(X_test)\n",
    "    ridge_val_mse = mean_squared_error(y_test, ridge_pred_val)\n",
    "    results_ridge.loc[i] = [ridge_train_score, ridge_val_score, ridge_train_mse, ridge_val_mse]\n",
    "\n",
    "results_ridge.index = ['alpha = 1', 'alpha = 100']\n",
    "print(\"Results for Ridge Regression: \")\n",
    "print(results_ridge)\n",
    "\n",
    "# Lasso regression\n",
    "for i in lasso_alpha_val:\n",
    "    lasso = Lasso(alpha = i).fit(X_train, y_train)\n",
    "    lasso_train_score = lasso.score(X_train, y_train)\n",
    "    lasso_val_score = lasso.score(X_test, y_test)\n",
    "    lasso_pred_train = lasso.predict(X_train)\n",
    "    lasso_train_mse = mean_squared_error(y_train, lasso_pred_train)\n",
    "    lasso_pred_val = lasso.predict(X_test)\n",
    "    lasso_val_mse = mean_squared_error(y_test, lasso_pred_val)\n",
    "    results_lasso.loc[i] = [lasso_train_score, lasso_val_score, lasso_train_mse, lasso_val_mse]\n",
    "    \n",
    "results_lasso.index = ['alpha = 1', 'alpha = 10']\n",
    "print(\"\\nResults for Lasso Regression: \")\n",
    "print(results_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b606236",
   "metadata": {},
   "source": [
    "*ANSWER HERE*\n",
    "\n",
    "The Lasso regression method with an alpha value of 10 gave the best R^2 score since it produced the highest validation accuracy value (0.626774). This score however, is not good enough as it is still relatively far from the maximum R^2 value of 1.0. Furthermore, the accuracy results did not see significant improvement when using Ridge or Lasso regression instead of Linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03b636e-eb68-482c-a691-708489fd2c64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
